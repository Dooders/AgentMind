# AgentMind

**AgentMind** is a modular cognitive framework for building interpretable, adaptive artificial agents. Each cognitive functionâ€”like attention, memory, inhibition, and reasoningâ€”is modeled as an independent module. This design allows researchers and developers to simulate humanlike behavior, evaluate decision-making processes, and study modular learning dynamics in artificial agents.

---

## Features

* ğŸ§  **Modular Cognition**: Plug-and-play modules for perception, memory, decision-making, and self-regulation.
* ğŸ“ˆ **Learning by Design**: Support for reinforcement, supervised, or rule-based learning inside each module.
* ğŸ” **Interpretability**: Track internal states and outputs of each module step-by-step.
* ğŸ§ª **Diagnostics Ready**: Built-in support for logging, introspection, and post-simulation assessment.
* ğŸ”„ **Evolvable Architecture**: Modules can be tuned, replaced, or evolved for different simulation needs.

---

## Example Modules

| Module             | Description                                    |
| ------------------ | ---------------------------------------------- |
| Attention Control  | Filters inputs based on salience and relevance |
| Working Memory     | Temporarily holds active concepts and data     |
| Impulse Inhibition | Blocks hasty or low-confidence actions         |
| Abstract Reasoning | Performs symbolic or conceptual inference      |
| Metacognition      | Self-monitors and tunes thresholds dynamically |
| Long-Term Memory   | Stores and retrieves persistent experience     |
| Goal Maintenance   | Manages long-term objectives across time       |

---

## Installation

```bash
git clone https://github.com/your-org/agentmind.git
cd agentmind
pip install -e .
```

---

## Running a Simulation

```python
from agentmind.core.agent import AgentMindCore
from agentmind.core.registry import register_all
from agentmind.envs.toy_env import ToyEnvironment

agent = AgentMindCore()
register_all(agent)

env = ToyEnvironment()

for step in range(100):
    obs = env.observe()
    action_id, decision = agent.step(obs)
    reward = env.evaluate(action_id, decision)
    agent.learn_from(action_id, reward)
```

---

## Code Structure

```graphql
agentmind/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py              # AgentMindCore and runtime loop
â”‚   â”œâ”€â”€ context.py            # Context/state object shared between modules
â”‚   â”œâ”€â”€ memory.py             # WorkingMemory, LongTermMemory
â”‚   â””â”€â”€ feedback.py           # Outcome tracking, delayed reward manager
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py               # MentalModule base class
â”‚   â”œâ”€â”€ attention.py
â”‚   â”œâ”€â”€ working_memory.py
â”‚   â”œâ”€â”€ inhibition.py
â”‚   â”œâ”€â”€ reasoning.py
â”‚   â”œâ”€â”€ flexibility.py
â”‚   â”œâ”€â”€ emotion.py
â”‚   â”œâ”€â”€ pattern_recognition.py
â”‚   â”œâ”€â”€ long_term_memory.py
â”‚   â”œâ”€â”€ metacognition.py
â”‚   â””â”€â”€ willpower.py
â”œâ”€â”€ learning/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rl.py                 # Reinforcement learning support
â”‚   â”œâ”€â”€ heuristics.py         # Simple rule/threshold updaters
â”‚   â””â”€â”€ evaluation.py         # Success metrics, logs
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ toy_env.py            # Minimal delayed-reward environment
â”‚   â””â”€â”€ gym_adapter.py        # Optional wrapper for OpenAI Gym
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py             # Logging, visualization, plots
â”‚   â””â”€â”€ persistence.py        # Save/load agent state, memory, config
â””â”€â”€ demo/
    â”œâ”€â”€ run_demo.py           # Runs a scripted scenario
    â””â”€â”€ config.json           # Demo parameters
```
